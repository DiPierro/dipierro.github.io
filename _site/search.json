[
  {
    "objectID": "who-is-amy-dipierro.html",
    "href": "who-is-amy-dipierro.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nHello\nMy name is Amy DiPierro. I am a data journalist reporting in the public interest.\nMy job is to look at the world and report what I see. In pursuit of that goal, I borrow liberally from fields like statistics and computer programming, and do the usual number-dialing and door-knocking, too.\nI work at EdSource, California’s largest education newsroom, where I lead our coverage of California State University. I was previously a data journalist at the Center for Public Integrity, where my reporting received an honorable mention in the Philip Meyer Journalism Award contest. While in graduate school at Stanford, I played a small role in a very cool Big Local News project called civic-scraper.\nYou can catch me discussing my work from time to time in places like KPBS and Capital Public Radio.\nI studied history and economics at Swarthmore College and hold a master’s degree in journalism from Stanford University, where I was a Knight-Hennessy Scholar.\n\n\nKudos\n\ntk\n\n\n\nSkills\n\nComputer programming and web scraping with Python\nData analysis with R and SQL\nStructured data extraction, categorization and more with large language models\nAutomation with GitHub Actions\nData visualization with Datawrapper and Flourish\nMap making with QGIS and R\nStatic site web development with Quarto\nVersion control with Git and GitHub\nPublic records requests"
  },
  {
    "objectID": "updates/welcome/index.html",
    "href": "updates/welcome/index.html",
    "title": "Code: Scraping DOGE",
    "section": "",
    "text": "I’ve been experimenting with GitHub Actions to automatically scrape the DOGE API once a week. This should make it possible to compare snapshots of each endpoint to one another over time.\nYou can clone the repository and run the script doge.sh from the command line to download a current snapshot of each endpoint as a JSON. Or you can access every JSON file that has already been scraped as a gist, which you can then download and analyze however you see fit!\nFor example, here’s how you might pull down the latest contracts JSON in R:\n\n# Libraries\nlibrary(tidyverse)\nlibrary(jsonlite)\n\nurl_gist &lt;- \"https://raw.githubusercontent.com/DiPierro/doge/refs/heads/main/contracts.json?token=GHSAT0AAAAAADFJ7N72SI3GWT2BBTY256562CHEOYA\"\nraw_contracts &lt;- fromJSON(curl::curl(url_gist))\ncontracts &lt;- map_dfr(raw_contracts, bind_rows)\n\ncontracts %&gt;%\n  head(5) %&gt;% \n  select(1:4) %&gt;% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\npiid\nagency\nvendor\nvalue\n\n\n\n\n140D0424F0005\nDepartment of the Interior\nFamily Endeavors, Inc\n3329900357\n\n\n2032H524A00020\nDepartment of Treasury\nCENTENNIAL TECHNOLOGIES INC.\n1900000000\n\n\nHT001523D0002\nDepartment of Defense\nA1FEDIMPACT\n1826530973\n\n\nFA872624FB071\nDepartment of Defense\nAccenture\n1491605888\n\n\nFA701420D0007\nDepartment of Air Force\nDeloitte Consulting LLP\n2750000000\n\n\n\n\n\nFound a bug? Got a comment, criticism or suggestion for improvement? Drop me a note: adipierro.edsource.org."
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Updates",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "updates/post-with-code/index.html",
    "href": "updates/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nHello\nMy name is Amy DiPierro. I am a data journalist reporting in the public interest.\nMy job is to look at the world and report what I see. In pursuit of that goal, I borrow liberally from fields like statistics and computer programming, and do the usual number-dialing and door-knocking, too.\nI work at EdSource, California’s largest education newsroom, where I lead our coverage of California State University. I was previously a data journalist at the Center for Public Integrity, where my reporting received an honorable mention in the Philip Meyer Journalism Award contest. While in graduate school at Stanford, I played a small role in a very cool Big Local News project called civic-scraper.\nI studied history and economics at Swarthmore College and hold a master’s degree in journalism from Stanford University, where I was a Knight-Hennessy Scholar.\nYou can catch me discussing my work in places like KPBS and Capital Public Radio, as well as journalism conferences like NICAR and IRE.\n\n\nKudos\n\nSpecial citation, Philip Meyer Journalism Award - Investigative Reporters & Editors (2024)\nShortlist - Sigma Awards (2023)\nBreaking Barriers Award - Institute for Nonprofit News (2023)\nDateline Awards - Winner, Online Series - Washington, D.C., Chapter of the Society of Professional Journalists (2023)\nStewart B. McKinney Award - Human Right to Housing Awards - National Homelessness Law Center (2023)\n\n\n\nSkills\n\nComputer programming and web scraping with Python\nData analysis with R and SQL\nStructured data extraction, categorization and more with large language models\nAutomation with GitHub Actions\nData visualization with Datawrapper and Flourish\nMap making with QGIS and R\nStatic site web development with Quarto\nVersion control with Git and GitHub\nPublic records requests"
  },
  {
    "objectID": "updates.html",
    "href": "updates.html",
    "title": "Updates",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJun 27, 2025\n\n\nCode: Visualizing New Jersey rest stops\n\n\ncode\n\n\n\n\nJun 8, 2025\n\n\nCode: Scraping DOGE\n\n\ncode\n\n\n\n\nJun 5, 2025\n\n\nListening: Marcia Griffiths, Yukihiro Takahashi, Pete Seeger\n\n\nlistening\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "work/post-with-code/index.html",
    "href": "work/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "work/welcome/index.html",
    "href": "work/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "updates/listening-2025-06/index.html",
    "href": "updates/listening-2025-06/index.html",
    "title": "Listening: Marcia Griffiths, Yukihiro Takahashi, Pete Seeger",
    "section": "",
    "text": "This is the first post in an occasional series sharing songs I heard and liked. Enjoy!\n Marcia Griffiths - “Everywhere”\n Yukihiro Takahashi - “Drip Dry Eyes”\n Pete Seeger - “The Dove”"
  },
  {
    "objectID": "updates/doge/index.html",
    "href": "updates/doge/index.html",
    "title": "Code: Scraping DOGE",
    "section": "",
    "text": "I’ve been experimenting with GitHub Actions to automatically scrape the DOGE API once a week. This should make it possible to compare snapshots of each endpoint to one another over time.\nYou can clone the repository and run the script doge.sh from the command line to download a JSON file of all the contracts, grants, leases or payments displayed on the DOGE website. Or you can grab the link to the most-recent JSON file the scraper has downloaded and analyze from there.\nFor example, here’s how you might pull down the latest contracts.json file using R:\n\n# Libraries\nlibrary(tidyverse)\nlibrary(jsonlite)\n\nurl_gist &lt;- \"https://raw.githubusercontent.com/DiPierro/doge/refs/heads/main/contracts.json\"\nraw_contracts &lt;- fromJSON(curl::curl(url_gist))\ncontracts &lt;- map_dfr(raw_contracts, bind_rows)\n\ncontracts %&gt;%\n  head(5) %&gt;% \n  select(1:4) %&gt;% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\npiid\nagency\nvendor\nvalue\n\n\n\n\n140D0424F0005\nDepartment of the Interior\nFamily Endeavors, Inc\n3329900357\n\n\n2032H524A00020\nDepartment of Treasury\nCENTENNIAL TECHNOLOGIES INC.\n1900000000\n\n\nHT001523D0002\nDepartment of Defense\nA1FEDIMPACT\n1826530973\n\n\nFA872624FB071\nDepartment of Defense\nAccenture\n1491605888\n\n\nFA701420D0007\nDepartment of Air Force\nDeloitte Consulting LLP\n2750000000\n\n\n\n\n\nA word of caution: several news reports have called out errors in DOGE’s accounting of its government restructuring efforts, so take care when working with this data. Here’s some further reading:\n\nThe New York Times: DOGE Makes Its Latest Errors Harder to Find\nCBS News: DOGE continues to publish misleading or inaccurate claims on its “Wall of Receipts”\n\nFound a bug? Got a comment, criticism or suggestion for improvement? Drop me a note: adipierro.edsource.org."
  },
  {
    "objectID": "work/2025-federal-research/index.html",
    "href": "work/2025-federal-research/index.html",
    "title": "Federal grant cuts hit California universities hard, putting research in limbo",
    "section": "",
    "text": "Noé C. Crespo, a professor of Health Promotion & Behavioral Science, poses outside the School of Public Health at San Diego State University.\n\n\n{{&lt; iframe https://datawrapper.dwcdn.net/a6AT9/2/ &gt;}}"
  },
  {
    "objectID": "updates/nj-rest-stops/index.html",
    "href": "updates/nj-rest-stops/index.html",
    "title": "Code: Visualizing New Jersey rest stops",
    "section": "",
    "text": "While road-tripping in New Jersey this summer, I put together this quick map showing all of the rest stops on the New Jersey Turnpike and Garden State Parkway – from the James Gandolfini Service Area in the northeast corner of the state, all the way down to the John Fenwick Service Plaza near Delaware.\n\n\n\n\n\n\nSources: NJGIN Open Data Portal, New Jersey Turnpike Authority, Wikipedia, Google Maps\nIf you find yourself traveling the same roads, you may also enjoy:\n\nGlenn Jones - “My Garden State”\nFountains of Wayne - “The Valley of Malls”\nBruce Springsteen - “Blue Highway”\n\nFound a bug? Got a comment, criticism or suggestion for improvement? Drop me a note: adipierro.edsource.org."
  },
  {
    "objectID": "work/2024-enrollment/index.html",
    "href": "work/2024-enrollment/index.html",
    "title": "Enrollment climbs at some Cal State campuses, tumbles at others",
    "section": "",
    "text": "I recently wrote about"
  }
]